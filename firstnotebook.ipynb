{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import configparser\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Dataset/datasets_894_813759_2015.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "\n",
    "\n",
    "dataset1_path = './Dataset/datasets_894_813759_2015.csv'\n",
    "dataset2_path = './Dataset/datasets_894_813759_2016.csv'\n",
    "dataset3_path = './Dataset/datasets_894_813759_2017.csv'\n",
    "dataset4_path = './Dataset/datasets_894_813759_2018.csv'\n",
    "dataset5_path = './Dataset/sample.txt'\n",
    "\n",
    "\n",
    "\n",
    "print(dataset1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-593f7b55e51b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m sc = SparkSession.builder.config('spark.jars.packages',\n\u001b[1;32m----> 2\u001b[1;33m                                     'org.apache.hadoop:hadoop-aws:2.6.4').getOrCreate()\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#                                      os.environ['AWS_ACCESS_KEY_ID'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 136\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\conf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[0;32m   1650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.config('spark.jars.packages',\n",
    "                                    'org.apache.hadoop:hadoop-aws:2.6.4').getOrCreate()\n",
    "\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "#                                      os.environ['AWS_ACCESS_KEY_ID'])\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\n",
    "#                                      os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"My app\")\n",
    "         .set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_matchscore = 1\n",
    "_missscore   = 0\n",
    "_firstgappenalty = -2\n",
    "_furthergappentaly = -1\n",
    "_topN = 20\n",
    "_minScoreToPrint = 1\n",
    "\n",
    "_gap = \"-\"\n",
    "_totalPairsEvaluated =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _GetScoreOnePair (s1, s2 ):\n",
    "    global _totalPairsEvaluated\n",
    "\n",
    "    thisscore = 0\n",
    "    firstgap = False\n",
    "    \n",
    "    m = min(len(s1), len(s2))\n",
    "    for pos in range (0, m):\n",
    "        if (s1[pos] == s2[pos]) and (s2[pos] != _gap) :\n",
    "            thisscore += _matchscore\n",
    "            firstgap = False\n",
    "        elif (s1[pos] == _gap) and ( s2[pos] != _gap ) :\n",
    "            if firstgap == True:\n",
    "                thisscore += _furthergappentaly\n",
    "            else:\n",
    "                firstgap = True\n",
    "                thisscore += _firstgappenalty\n",
    "        elif (s2[pos] == _gap) and ( s1[pos] != _gap ) :\n",
    "            if firstgap == True:\n",
    "                thisscore += _furthergappentaly\n",
    "            else:\n",
    "                firstgap = True\n",
    "                thisscore += _firstgappenalty\n",
    "        else :\n",
    "            firstgap = False\n",
    "            thisscore += _missscore\n",
    "\n",
    "    _totalPairsEvaluated = _totalPairsEvaluated + 1\n",
    "    if ( _totalPairsEvaluated%100000) == 0 :\n",
    "        print (_totalPairsEvaluated)\n",
    "        \n",
    "    return thisscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  _OutputProteinPairWithScore (_sl1, _sl2, score):\n",
    "    str1 = _sl1.rstrip()\n",
    "    str2 = _sl2.rstrip()\n",
    "    m = max(len(str1), len(str2));\n",
    "\n",
    "\n",
    "    if score >= _minScoreToPrint:\n",
    "        print (\"pos\", end = \" \" )\n",
    "        for pos in range (0, m):\n",
    "            print(pos, end = \" \")\n",
    "        print(\"  score = \", score)\n",
    "\n",
    "        print (\"   \", end = \" \" )\n",
    "        for pos in range (0, len(str1)):\n",
    "            print (str1[pos], end = \" \" )\n",
    "        print(\"\")\n",
    "\n",
    "        print (\"   \", end = \" \" )\n",
    "        for pos in range (0, len(str2)):\n",
    "            print (str2[pos], end = \" \" )\n",
    "        print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _CreateProteinVariantsStartingSpaces ( _s1, _diff_len ):\n",
    "    result = []\n",
    "    result.append(_s1)\n",
    "    \n",
    "    for i in range (1, _diff_len):\n",
    "        s = _s1.rjust(len(_s1)+i)\n",
    "        result.append(s)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _CreateProteinVariantsWithGap (_sl1, _num_gaps, _max_num_gaps, _min_index, _max_index ):\n",
    "    strings = []\n",
    "    if  _num_gaps == _max_num_gaps:\n",
    "        strings.append(_sl1)\n",
    "        \n",
    "    strinlen = len(_sl1)\n",
    "    if strinlen >  _max_index:\n",
    "        strinlen =  _max_index\n",
    "        \n",
    "    #do not add a gap at the beginning and the end    \n",
    "    for i in range (_min_index+1, strinlen-1):\n",
    "        stl = _sl1[0:i] + _gap + _sl1[i:];\n",
    "        strings.append(stl)\n",
    "        #print (_min_index, _max_index, stl)\n",
    "        if  _num_gaps > 1 :\n",
    "            stl2 = _CreateProteinVariantsWithGap(stl, _num_gaps-1, _max_num_gaps,  _min_index, _max_index )\n",
    "            for s in stl2:\n",
    "                strings.append(s)\n",
    "            \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _GetScoresWithVariants ( _sl1, _sl2, max_gaps ):\n",
    "    currentmin = -1000 # Current minimum value to be added to the vec\n",
    "    currentmax = -1000 # Maximum sore observed so far\n",
    "    scorevec = []\n",
    "    s1vec    = []\n",
    "    \n",
    "    s1 = _sl1.upper()\n",
    "    s2 = _sl2.upper()\n",
    "\n",
    "\n",
    "    diff_len = len(s2) - len(s1)\n",
    "    if diff_len < 0 :\n",
    "        diff_len = 0\n",
    "    s1x = _CreateProteinVariantsStartingSpaces (s1, diff_len )\n",
    "    for s in s1x:\n",
    "        max_len = min (len(s), len(s2)) + max_gaps\n",
    "        min_index = 0\n",
    "        for j in range (0, len(s) ):\n",
    "            if s[j] == \" \":\n",
    "                min_index = min_index + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        s1var = _CreateProteinVariantsWithGap(s, max_gaps, max_gaps, min_index, max_len )\n",
    "        for sx in s1var:\n",
    "            s1vec.append(sx)\n",
    "            \n",
    "    #String s2 is assumed to be from a database, so we do not calculate variants on it\n",
    "\n",
    "    #remove duplicates\n",
    "    s1variants = list(dict.fromkeys(s1vec))\n",
    "    i = 0\n",
    "    for s1var in s1variants:\n",
    "        score = _GetScoreOnePair (s1var, s2 )\n",
    "        tup = (score, s1var, s2)\n",
    "        if i < _topN:\n",
    "            scorevec.append(tup)\n",
    "            if  i == 0:\n",
    "                currentmin = score\n",
    "            elif  score < currentmin:\n",
    "                currentmin = score\n",
    "                if i == 0 :\n",
    "                    currentmax = score\n",
    "                elif score > currentmax:\n",
    "                    currentmax = score\n",
    "        else:\n",
    "            if  score > currentmin:\n",
    "                scorevec.append(tup)\n",
    "        i = i+1\n",
    "        scorevec.sort(key = lambda x : -x[0]);\n",
    "        scorevec = scorevec[0:_topN]\n",
    "        currentmin = min (scorevec)[0]\n",
    "        currentmax = max (scorevec)[0]\n",
    "            \n",
    "    return scorevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HIK8PU2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My app>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage : python3 proteinscoring-with-test.py proteinfile.txt dbfile.txt max_gaps\n",
      "Total Pairs evaluated  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Usage : python3 proteinscoring-with-test.py proteinfile.txt dbfile.txt max_gaps\")\n",
    "allvec = []\n",
    "\n",
    "num_entries_in_db = 0\n",
    "\n",
    "# var_pro=protein_seq.collect()\n",
    "#dbfile = sys.argv[2] old  code\n",
    "#dbfile = sc.textFile(\"./dbnew.txt\")\n",
    "dbfile=sc.parallelize(['abcdef','ghijk','abcdeg'])\n",
    "# dbfile.cache()\n",
    "dbsample = dbfile.map(lambda w : ('abc',w,2))\n",
    "# dbsample1=dbsample.map(lambda w:(w,max_gaps))\n",
    "max_gaps = 2\n",
    "#dbsample1.collect()\n",
    "#     for dbsequence in open(dbfile, 'r').readlines():  old code\n",
    "#         num_entries_in_db = num_entries_in_db + 1    olld code\n",
    "#scorevec = _GetScoresWithVariants ( proteinsequence, dbsequence, max_gaps) # old code\n",
    "#scorevec=dbsample.map(lambda x:  _GetScoresWithVariants(x[0],x[1],x[2]))\n",
    "scorevec=dbsample.flatMap(lambda x:  _GetScoresWithVariants(x[0],x[1],x[2]))\n",
    "#print(\"scorevec obtained \",scorevec)\n",
    "#     for s in scorevec:     old  code not needed\n",
    "#         allvec.append(s)\n",
    "\n",
    "\n",
    "#     allvec.sort (key = lambda x : -x[0] ) old code\n",
    "#     allvec = allvec [0:_topN]    old code\n",
    "#  scorevec.sort (key = lambda x : -x[0] ) \n",
    "# scorevec = scorevec [0:_topN] \n",
    "\n",
    "#for tup in allvec: old code\n",
    "#     for tup in scorevec:    old  code\n",
    "\n",
    "#         _OutputProteinPairWithScore (tup[1], tup[2], tup[0])  old code\n",
    "#     finalans=scorevec.map(lambda y:_OutputProteinPairWithScore (y[1], y[2], y[0]))   \n",
    "#print(\"Number of entries in database \" , num_entries_in_db)    \n",
    "print(\"Total Pairs evaluated \" , _totalPairsEvaluated) \n",
    "#print(\"final ans\",finalans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'ABC', 'ABCDEF'),\n",
       " (0, ' ABC', 'ABCDEF'),\n",
       " (0, '  ABC', 'ABCDEF'),\n",
       " (-1, 'A-BC', 'ABCDEF'),\n",
       " (-2, 'A--BC', 'ABCDEF'),\n",
       " (-2, ' A-BC', 'ABCDEF'),\n",
       " (-2, '  A-BC', 'ABCDEF'),\n",
       " (-3, ' A--BC', 'ABCDEF'),\n",
       " (-3, '  A--BC', 'ABCDEF'),\n",
       " (0, 'ABC', 'GHIJK'),\n",
       " (0, ' ABC', 'GHIJK'),\n",
       " (-2, 'A-BC', 'GHIJK'),\n",
       " (-2, ' A-BC', 'GHIJK'),\n",
       " (-3, 'A--BC', 'GHIJK'),\n",
       " (-3, ' A--BC', 'GHIJK'),\n",
       " (3, 'ABC', 'ABCDEG'),\n",
       " (0, ' ABC', 'ABCDEG'),\n",
       " (0, '  ABC', 'ABCDEG'),\n",
       " (-1, 'A-BC', 'ABCDEG'),\n",
       " (-2, 'A--BC', 'ABCDEG'),\n",
       " (-2, ' A-BC', 'ABCDEG'),\n",
       " (-2, '  A-BC', 'ABCDEG'),\n",
       " (-3, ' A--BC', 'ABCDEG'),\n",
       " (-3, '  A--BC', 'ABCDEG')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorevec.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorevec.saveAsTextFile(\"./results_ssds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = sc.parallelize([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o68.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f17f9ca06afc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./results_so\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o68.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "RDD\n",
    "RDD.saveAsTextFile(\"./results_so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9c211f0e0661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./results123\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1570\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 41 more\r\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:140)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "RDD.saveAsTextFile(\"./results123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_1_data = sc.read.csv(dataset1_path, header=True)\n",
    "# dataset_2_data = sc.read.csv(dataset2_path, header=True)\n",
    "# dataset_3_data = sc.read.csv(dataset3_path, header=True)\n",
    "# dataset_4_data = sc.read.csv(dataset4_path, header=True)\n",
    "data = sc.textFile(dataset5_path)\n",
    "# dataset_5_data = sc.read.csv(dataset5_path, header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data.flatMap(lambda line: line.split(\" \"))\n",
    "wordscount = words.map(lambda word : (word, 1)).reduceByKey(lambda a, b : a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HIK8PU2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My app>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_data = dataset_1_data.withColumn(\"Year\", lit(\"2015\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2_data = dataset_2_data.withColumn(\"Year\", lit(\"2016\"))\n",
    "dataset_3_data = dataset_3_data.withColumn(\"Year\", lit(\"2017\"))\n",
    "dataset_4_data = dataset_4_data.withColumn(\"Year\", lit(\"2018\"))\n",
    "# dataset_5_data = dataset_5_data.withColumn(\"Year\", lit(\"2019\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Happiness_Rank</th>\n",
       "      <th>Happiness_Score</th>\n",
       "      <th>Standard_Error</th>\n",
       "      <th>Economy (GDP per Capita)</th>\n",
       "      <th>Family</th>\n",
       "      <th>Health(Life_Expectancy)</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Trust(Government_Corruption)</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Dystopia_Residual</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>1</td>\n",
       "      <td>7.587</td>\n",
       "      <td>0.03411</td>\n",
       "      <td>1.39651</td>\n",
       "      <td>1.34951</td>\n",
       "      <td>0.94143</td>\n",
       "      <td>0.66557</td>\n",
       "      <td>0.41978</td>\n",
       "      <td>0.29678</td>\n",
       "      <td>2.51738</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>2</td>\n",
       "      <td>7.561</td>\n",
       "      <td>0.04884</td>\n",
       "      <td>1.30232</td>\n",
       "      <td>1.40223</td>\n",
       "      <td>0.94784</td>\n",
       "      <td>0.62877</td>\n",
       "      <td>0.14145</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>2.70201</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>3</td>\n",
       "      <td>7.527</td>\n",
       "      <td>0.03328</td>\n",
       "      <td>1.32548</td>\n",
       "      <td>1.36058</td>\n",
       "      <td>0.87464</td>\n",
       "      <td>0.64938</td>\n",
       "      <td>0.48357</td>\n",
       "      <td>0.34139</td>\n",
       "      <td>2.49204</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>4</td>\n",
       "      <td>7.522</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>1.459</td>\n",
       "      <td>1.33095</td>\n",
       "      <td>0.88521</td>\n",
       "      <td>0.66973</td>\n",
       "      <td>0.36503</td>\n",
       "      <td>0.34699</td>\n",
       "      <td>2.46531</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>5</td>\n",
       "      <td>7.427</td>\n",
       "      <td>0.03553</td>\n",
       "      <td>1.32629</td>\n",
       "      <td>1.32261</td>\n",
       "      <td>0.90563</td>\n",
       "      <td>0.63297</td>\n",
       "      <td>0.32957</td>\n",
       "      <td>0.45811</td>\n",
       "      <td>2.45176</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>6</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>1.29025</td>\n",
       "      <td>1.31826</td>\n",
       "      <td>0.88911</td>\n",
       "      <td>0.64169</td>\n",
       "      <td>0.41372</td>\n",
       "      <td>0.23351</td>\n",
       "      <td>2.61955</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7</td>\n",
       "      <td>7.378</td>\n",
       "      <td>0.02799</td>\n",
       "      <td>1.32944</td>\n",
       "      <td>1.28017</td>\n",
       "      <td>0.89284</td>\n",
       "      <td>0.61576</td>\n",
       "      <td>0.31814</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>2.4657</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>8</td>\n",
       "      <td>7.364</td>\n",
       "      <td>0.03157</td>\n",
       "      <td>1.33171</td>\n",
       "      <td>1.28907</td>\n",
       "      <td>0.91087</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>0.43844</td>\n",
       "      <td>0.36262</td>\n",
       "      <td>2.37119</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "      <td>9</td>\n",
       "      <td>7.286</td>\n",
       "      <td>0.03371</td>\n",
       "      <td>1.25018</td>\n",
       "      <td>1.31967</td>\n",
       "      <td>0.90837</td>\n",
       "      <td>0.63938</td>\n",
       "      <td>0.42922</td>\n",
       "      <td>0.47501</td>\n",
       "      <td>2.26425</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "      <td>10</td>\n",
       "      <td>7.284</td>\n",
       "      <td>0.04083</td>\n",
       "      <td>1.33358</td>\n",
       "      <td>1.30923</td>\n",
       "      <td>0.93156</td>\n",
       "      <td>0.65124</td>\n",
       "      <td>0.35637</td>\n",
       "      <td>0.43562</td>\n",
       "      <td>2.26646</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Israel</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>11</td>\n",
       "      <td>7.278</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>1.22857</td>\n",
       "      <td>1.22393</td>\n",
       "      <td>0.91387</td>\n",
       "      <td>0.41319</td>\n",
       "      <td>0.07785</td>\n",
       "      <td>0.33172</td>\n",
       "      <td>3.08854</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>12</td>\n",
       "      <td>7.226</td>\n",
       "      <td>0.04454</td>\n",
       "      <td>0.95578</td>\n",
       "      <td>1.23788</td>\n",
       "      <td>0.86027</td>\n",
       "      <td>0.63376</td>\n",
       "      <td>0.10583</td>\n",
       "      <td>0.25497</td>\n",
       "      <td>3.17728</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>13</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.03751</td>\n",
       "      <td>1.33723</td>\n",
       "      <td>1.29704</td>\n",
       "      <td>0.89042</td>\n",
       "      <td>0.62433</td>\n",
       "      <td>0.18676</td>\n",
       "      <td>0.33088</td>\n",
       "      <td>2.5332</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>14</td>\n",
       "      <td>7.187</td>\n",
       "      <td>0.04176</td>\n",
       "      <td>1.02054</td>\n",
       "      <td>0.91451</td>\n",
       "      <td>0.81444</td>\n",
       "      <td>0.48181</td>\n",
       "      <td>0.21312</td>\n",
       "      <td>0.14074</td>\n",
       "      <td>3.60214</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>15</td>\n",
       "      <td>7.119</td>\n",
       "      <td>0.03839</td>\n",
       "      <td>1.39451</td>\n",
       "      <td>1.24711</td>\n",
       "      <td>0.86179</td>\n",
       "      <td>0.54604</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>0.40105</td>\n",
       "      <td>2.51011</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>16</td>\n",
       "      <td>6.983</td>\n",
       "      <td>0.04076</td>\n",
       "      <td>0.98124</td>\n",
       "      <td>1.23287</td>\n",
       "      <td>0.69702</td>\n",
       "      <td>0.49049</td>\n",
       "      <td>0.17521</td>\n",
       "      <td>0.14574</td>\n",
       "      <td>3.26001</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>17</td>\n",
       "      <td>6.946</td>\n",
       "      <td>0.03499</td>\n",
       "      <td>1.56391</td>\n",
       "      <td>1.21963</td>\n",
       "      <td>0.91894</td>\n",
       "      <td>0.61583</td>\n",
       "      <td>0.37798</td>\n",
       "      <td>0.28034</td>\n",
       "      <td>1.96961</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>18</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.03676</td>\n",
       "      <td>1.33596</td>\n",
       "      <td>1.36948</td>\n",
       "      <td>0.89533</td>\n",
       "      <td>0.61777</td>\n",
       "      <td>0.28703</td>\n",
       "      <td>0.45901</td>\n",
       "      <td>1.9757</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>19</td>\n",
       "      <td>6.937</td>\n",
       "      <td>0.03595</td>\n",
       "      <td>1.30782</td>\n",
       "      <td>1.28566</td>\n",
       "      <td>0.89667</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>2.41484</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>20</td>\n",
       "      <td>6.901</td>\n",
       "      <td>0.03729</td>\n",
       "      <td>1.42727</td>\n",
       "      <td>1.12575</td>\n",
       "      <td>0.80925</td>\n",
       "      <td>0.64157</td>\n",
       "      <td>0.38583</td>\n",
       "      <td>0.26428</td>\n",
       "      <td>2.24743</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>21</td>\n",
       "      <td>6.867</td>\n",
       "      <td>0.01866</td>\n",
       "      <td>1.26637</td>\n",
       "      <td>1.28548</td>\n",
       "      <td>0.90943</td>\n",
       "      <td>0.59625</td>\n",
       "      <td>0.32067</td>\n",
       "      <td>0.51912</td>\n",
       "      <td>1.96994</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Oman</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>22</td>\n",
       "      <td>6.853</td>\n",
       "      <td>0.05335</td>\n",
       "      <td>1.36011</td>\n",
       "      <td>1.08182</td>\n",
       "      <td>0.76276</td>\n",
       "      <td>0.63274</td>\n",
       "      <td>0.32524</td>\n",
       "      <td>0.21542</td>\n",
       "      <td>2.47489</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>23</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.06476</td>\n",
       "      <td>1.04424</td>\n",
       "      <td>1.25596</td>\n",
       "      <td>0.72052</td>\n",
       "      <td>0.42908</td>\n",
       "      <td>0.11069</td>\n",
       "      <td>0.05841</td>\n",
       "      <td>3.19131</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Southeastern Asia</td>\n",
       "      <td>24</td>\n",
       "      <td>6.798</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>1.52186</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02525</td>\n",
       "      <td>0.54252</td>\n",
       "      <td>0.4921</td>\n",
       "      <td>0.31105</td>\n",
       "      <td>1.88501</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Panama</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>25</td>\n",
       "      <td>6.786</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>1.06353</td>\n",
       "      <td>1.1985</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>0.5421</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.24434</td>\n",
       "      <td>2.84848</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>26</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.01848</td>\n",
       "      <td>1.32792</td>\n",
       "      <td>1.29937</td>\n",
       "      <td>0.89186</td>\n",
       "      <td>0.61477</td>\n",
       "      <td>0.21843</td>\n",
       "      <td>0.28214</td>\n",
       "      <td>2.11569</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chile</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>27</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.10715</td>\n",
       "      <td>1.12447</td>\n",
       "      <td>0.85857</td>\n",
       "      <td>0.44132</td>\n",
       "      <td>0.12869</td>\n",
       "      <td>0.33363</td>\n",
       "      <td>2.67585</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Qatar</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>28</td>\n",
       "      <td>6.611</td>\n",
       "      <td>0.06257</td>\n",
       "      <td>1.69042</td>\n",
       "      <td>1.0786</td>\n",
       "      <td>0.79733</td>\n",
       "      <td>0.6404</td>\n",
       "      <td>0.52208</td>\n",
       "      <td>0.32573</td>\n",
       "      <td>1.55674</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>France</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>29</td>\n",
       "      <td>6.575</td>\n",
       "      <td>0.03512</td>\n",
       "      <td>1.27778</td>\n",
       "      <td>1.26038</td>\n",
       "      <td>0.94579</td>\n",
       "      <td>0.55011</td>\n",
       "      <td>0.20646</td>\n",
       "      <td>0.12332</td>\n",
       "      <td>2.21126</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "      <td>30</td>\n",
       "      <td>6.574</td>\n",
       "      <td>0.04612</td>\n",
       "      <td>1.05351</td>\n",
       "      <td>1.24823</td>\n",
       "      <td>0.78723</td>\n",
       "      <td>0.44974</td>\n",
       "      <td>0.08484</td>\n",
       "      <td>0.11451</td>\n",
       "      <td>2.836</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Myanmar</td>\n",
       "      <td>Southeastern Asia</td>\n",
       "      <td>129</td>\n",
       "      <td>4.307</td>\n",
       "      <td>0.04351</td>\n",
       "      <td>0.27108</td>\n",
       "      <td>0.70905</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>0.44017</td>\n",
       "      <td>0.19034</td>\n",
       "      <td>0.79588</td>\n",
       "      <td>1.41805</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "      <td>130</td>\n",
       "      <td>4.297</td>\n",
       "      <td>0.04221</td>\n",
       "      <td>0.7419</td>\n",
       "      <td>0.38562</td>\n",
       "      <td>0.72926</td>\n",
       "      <td>0.40577</td>\n",
       "      <td>0.38331</td>\n",
       "      <td>0.05547</td>\n",
       "      <td>1.59541</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Malawi</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>131</td>\n",
       "      <td>4.292</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.01604</td>\n",
       "      <td>0.41134</td>\n",
       "      <td>0.22562</td>\n",
       "      <td>0.43054</td>\n",
       "      <td>0.06977</td>\n",
       "      <td>0.33128</td>\n",
       "      <td>2.80791</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>132</td>\n",
       "      <td>4.271</td>\n",
       "      <td>0.03751</td>\n",
       "      <td>0.83524</td>\n",
       "      <td>1.01905</td>\n",
       "      <td>0.70806</td>\n",
       "      <td>0.53726</td>\n",
       "      <td>0.09179</td>\n",
       "      <td>0.40828</td>\n",
       "      <td>0.67108</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Cameroon</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>133</td>\n",
       "      <td>4.252</td>\n",
       "      <td>0.04678</td>\n",
       "      <td>0.4225</td>\n",
       "      <td>0.88767</td>\n",
       "      <td>0.23402</td>\n",
       "      <td>0.49309</td>\n",
       "      <td>0.05786</td>\n",
       "      <td>0.20618</td>\n",
       "      <td>1.95071</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "      <td>134</td>\n",
       "      <td>4.218</td>\n",
       "      <td>0.04828</td>\n",
       "      <td>1.01216</td>\n",
       "      <td>1.10614</td>\n",
       "      <td>0.76649</td>\n",
       "      <td>0.30587</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.11921</td>\n",
       "      <td>0.89991</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>135</td>\n",
       "      <td>4.194</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.61712</td>\n",
       "      <td>0.17288</td>\n",
       "      <td>0.06324</td>\n",
       "      <td>0.11291</td>\n",
       "      <td>1.59927</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>136</td>\n",
       "      <td>4.077</td>\n",
       "      <td>0.04367</td>\n",
       "      <td>0.54649</td>\n",
       "      <td>0.68093</td>\n",
       "      <td>0.40064</td>\n",
       "      <td>0.35571</td>\n",
       "      <td>0.07854</td>\n",
       "      <td>0.09131</td>\n",
       "      <td>1.92313</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>137</td>\n",
       "      <td>4.033</td>\n",
       "      <td>0.04758</td>\n",
       "      <td>0.75778</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.16683</td>\n",
       "      <td>0.10384</td>\n",
       "      <td>0.07122</td>\n",
       "      <td>0.12344</td>\n",
       "      <td>1.94939</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Mali</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>138</td>\n",
       "      <td>3.995</td>\n",
       "      <td>0.05602</td>\n",
       "      <td>0.26074</td>\n",
       "      <td>1.03526</td>\n",
       "      <td>0.20583</td>\n",
       "      <td>0.38857</td>\n",
       "      <td>0.12352</td>\n",
       "      <td>0.18798</td>\n",
       "      <td>1.79293</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Congo (Brazzaville)</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>139</td>\n",
       "      <td>3.989</td>\n",
       "      <td>0.06682</td>\n",
       "      <td>0.67866</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>0.31051</td>\n",
       "      <td>0.41466</td>\n",
       "      <td>0.11686</td>\n",
       "      <td>0.12388</td>\n",
       "      <td>1.68135</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Comoros</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>140</td>\n",
       "      <td>3.956</td>\n",
       "      <td>0.04797</td>\n",
       "      <td>0.23906</td>\n",
       "      <td>0.79273</td>\n",
       "      <td>0.36315</td>\n",
       "      <td>0.22917</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.17441</td>\n",
       "      <td>1.95812</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Uganda</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>141</td>\n",
       "      <td>3.931</td>\n",
       "      <td>0.04317</td>\n",
       "      <td>0.21102</td>\n",
       "      <td>1.13299</td>\n",
       "      <td>0.33861</td>\n",
       "      <td>0.45727</td>\n",
       "      <td>0.07267</td>\n",
       "      <td>0.29066</td>\n",
       "      <td>1.42766</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Senegal</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>142</td>\n",
       "      <td>3.904</td>\n",
       "      <td>0.03608</td>\n",
       "      <td>0.36498</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0.36772</td>\n",
       "      <td>0.10713</td>\n",
       "      <td>0.20843</td>\n",
       "      <td>1.44395</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Gabon</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>143</td>\n",
       "      <td>3.896</td>\n",
       "      <td>0.04547</td>\n",
       "      <td>1.06024</td>\n",
       "      <td>0.90528</td>\n",
       "      <td>0.43372</td>\n",
       "      <td>0.31914</td>\n",
       "      <td>0.11091</td>\n",
       "      <td>0.06822</td>\n",
       "      <td>0.99895</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Niger</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>144</td>\n",
       "      <td>3.845</td>\n",
       "      <td>0.03602</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.77265</td>\n",
       "      <td>0.29707</td>\n",
       "      <td>0.47692</td>\n",
       "      <td>0.15639</td>\n",
       "      <td>0.19387</td>\n",
       "      <td>1.87877</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Cambodia</td>\n",
       "      <td>Southeastern Asia</td>\n",
       "      <td>145</td>\n",
       "      <td>3.819</td>\n",
       "      <td>0.05069</td>\n",
       "      <td>0.46038</td>\n",
       "      <td>0.62736</td>\n",
       "      <td>0.61114</td>\n",
       "      <td>0.66246</td>\n",
       "      <td>0.07247</td>\n",
       "      <td>0.40359</td>\n",
       "      <td>0.98195</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>146</td>\n",
       "      <td>3.781</td>\n",
       "      <td>0.05061</td>\n",
       "      <td>0.2852</td>\n",
       "      <td>1.00268</td>\n",
       "      <td>0.38215</td>\n",
       "      <td>0.32878</td>\n",
       "      <td>0.05747</td>\n",
       "      <td>0.34377</td>\n",
       "      <td>1.38079</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Madagascar</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>147</td>\n",
       "      <td>3.681</td>\n",
       "      <td>0.03633</td>\n",
       "      <td>0.20824</td>\n",
       "      <td>0.66801</td>\n",
       "      <td>0.46721</td>\n",
       "      <td>0.19184</td>\n",
       "      <td>0.08124</td>\n",
       "      <td>0.21333</td>\n",
       "      <td>1.851</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>148</td>\n",
       "      <td>3.678</td>\n",
       "      <td>0.06112</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06699</td>\n",
       "      <td>0.48879</td>\n",
       "      <td>0.08289</td>\n",
       "      <td>0.23835</td>\n",
       "      <td>2.7223</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Chad</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>149</td>\n",
       "      <td>3.667</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.34193</td>\n",
       "      <td>0.76062</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>0.23501</td>\n",
       "      <td>0.05269</td>\n",
       "      <td>0.18386</td>\n",
       "      <td>1.94296</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>150</td>\n",
       "      <td>3.656</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.17417</td>\n",
       "      <td>0.46475</td>\n",
       "      <td>0.24009</td>\n",
       "      <td>0.37725</td>\n",
       "      <td>0.12139</td>\n",
       "      <td>0.28657</td>\n",
       "      <td>1.99172</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Ivory Coast</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>151</td>\n",
       "      <td>3.655</td>\n",
       "      <td>0.05141</td>\n",
       "      <td>0.46534</td>\n",
       "      <td>0.77115</td>\n",
       "      <td>0.15185</td>\n",
       "      <td>0.46866</td>\n",
       "      <td>0.17922</td>\n",
       "      <td>0.20165</td>\n",
       "      <td>1.41723</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>152</td>\n",
       "      <td>3.587</td>\n",
       "      <td>0.04324</td>\n",
       "      <td>0.25812</td>\n",
       "      <td>0.85188</td>\n",
       "      <td>0.27125</td>\n",
       "      <td>0.39493</td>\n",
       "      <td>0.12832</td>\n",
       "      <td>0.21747</td>\n",
       "      <td>1.46494</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>153</td>\n",
       "      <td>3.575</td>\n",
       "      <td>0.03084</td>\n",
       "      <td>0.31982</td>\n",
       "      <td>0.30285</td>\n",
       "      <td>0.30335</td>\n",
       "      <td>0.23414</td>\n",
       "      <td>0.09719</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>1.9521</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Rwanda</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>154</td>\n",
       "      <td>3.465</td>\n",
       "      <td>0.03464</td>\n",
       "      <td>0.22208</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>0.42864</td>\n",
       "      <td>0.59201</td>\n",
       "      <td>0.55191</td>\n",
       "      <td>0.22628</td>\n",
       "      <td>0.67042</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Benin</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>155</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.03656</td>\n",
       "      <td>0.28665</td>\n",
       "      <td>0.35386</td>\n",
       "      <td>0.3191</td>\n",
       "      <td>0.4845</td>\n",
       "      <td>0.0801</td>\n",
       "      <td>0.1826</td>\n",
       "      <td>1.63328</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Syria</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>156</td>\n",
       "      <td>3.006</td>\n",
       "      <td>0.05015</td>\n",
       "      <td>0.6632</td>\n",
       "      <td>0.47489</td>\n",
       "      <td>0.72193</td>\n",
       "      <td>0.15684</td>\n",
       "      <td>0.18906</td>\n",
       "      <td>0.47179</td>\n",
       "      <td>0.32858</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>157</td>\n",
       "      <td>2.905</td>\n",
       "      <td>0.08658</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.41587</td>\n",
       "      <td>0.22396</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.10062</td>\n",
       "      <td>0.19727</td>\n",
       "      <td>1.83302</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Togo</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>158</td>\n",
       "      <td>2.839</td>\n",
       "      <td>0.06727</td>\n",
       "      <td>0.20868</td>\n",
       "      <td>0.13995</td>\n",
       "      <td>0.28443</td>\n",
       "      <td>0.36453</td>\n",
       "      <td>0.10731</td>\n",
       "      <td>0.16681</td>\n",
       "      <td>1.56726</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Country                           Region Happiness_Rank  \\\n",
       "0                 Switzerland                   Western Europe              1   \n",
       "1                     Iceland                   Western Europe              2   \n",
       "2                     Denmark                   Western Europe              3   \n",
       "3                      Norway                   Western Europe              4   \n",
       "4                      Canada                    North America              5   \n",
       "5                     Finland                   Western Europe              6   \n",
       "6                 Netherlands                   Western Europe              7   \n",
       "7                      Sweden                   Western Europe              8   \n",
       "8                 New Zealand        Australia and New Zealand              9   \n",
       "9                   Australia        Australia and New Zealand             10   \n",
       "10                     Israel  Middle East and Northern Africa             11   \n",
       "11                 Costa Rica      Latin America and Caribbean             12   \n",
       "12                    Austria                   Western Europe             13   \n",
       "13                     Mexico      Latin America and Caribbean             14   \n",
       "14              United States                    North America             15   \n",
       "15                     Brazil      Latin America and Caribbean             16   \n",
       "16                 Luxembourg                   Western Europe             17   \n",
       "17                    Ireland                   Western Europe             18   \n",
       "18                    Belgium                   Western Europe             19   \n",
       "19       United Arab Emirates  Middle East and Northern Africa             20   \n",
       "20             United Kingdom                   Western Europe             21   \n",
       "21                       Oman  Middle East and Northern Africa             22   \n",
       "22                  Venezuela      Latin America and Caribbean             23   \n",
       "23                  Singapore                Southeastern Asia             24   \n",
       "24                     Panama      Latin America and Caribbean             25   \n",
       "25                    Germany                   Western Europe             26   \n",
       "26                      Chile      Latin America and Caribbean             27   \n",
       "27                      Qatar  Middle East and Northern Africa             28   \n",
       "28                     France                   Western Europe             29   \n",
       "29                  Argentina      Latin America and Caribbean             30   \n",
       "..                        ...                              ...            ...   \n",
       "128                   Myanmar                Southeastern Asia            129   \n",
       "129                   Georgia       Central and Eastern Europe            130   \n",
       "130                    Malawi               Sub-Saharan Africa            131   \n",
       "131                 Sri Lanka                    Southern Asia            132   \n",
       "132                  Cameroon               Sub-Saharan Africa            133   \n",
       "133                  Bulgaria       Central and Eastern Europe            134   \n",
       "134                     Egypt  Middle East and Northern Africa            135   \n",
       "135                     Yemen  Middle East and Northern Africa            136   \n",
       "136                    Angola               Sub-Saharan Africa            137   \n",
       "137                      Mali               Sub-Saharan Africa            138   \n",
       "138       Congo (Brazzaville)               Sub-Saharan Africa            139   \n",
       "139                   Comoros               Sub-Saharan Africa            140   \n",
       "140                    Uganda               Sub-Saharan Africa            141   \n",
       "141                   Senegal               Sub-Saharan Africa            142   \n",
       "142                     Gabon               Sub-Saharan Africa            143   \n",
       "143                     Niger               Sub-Saharan Africa            144   \n",
       "144                  Cambodia                Southeastern Asia            145   \n",
       "145                  Tanzania               Sub-Saharan Africa            146   \n",
       "146                Madagascar               Sub-Saharan Africa            147   \n",
       "147  Central African Republic               Sub-Saharan Africa            148   \n",
       "148                      Chad               Sub-Saharan Africa            149   \n",
       "149                    Guinea               Sub-Saharan Africa            150   \n",
       "150               Ivory Coast               Sub-Saharan Africa            151   \n",
       "151              Burkina Faso               Sub-Saharan Africa            152   \n",
       "152               Afghanistan                    Southern Asia            153   \n",
       "153                    Rwanda               Sub-Saharan Africa            154   \n",
       "154                     Benin               Sub-Saharan Africa            155   \n",
       "155                     Syria  Middle East and Northern Africa            156   \n",
       "156                   Burundi               Sub-Saharan Africa            157   \n",
       "157                      Togo               Sub-Saharan Africa            158   \n",
       "\n",
       "    Happiness_Score Standard_Error Economy (GDP per Capita)   Family  \\\n",
       "0             7.587        0.03411                  1.39651  1.34951   \n",
       "1             7.561        0.04884                  1.30232  1.40223   \n",
       "2             7.527        0.03328                  1.32548  1.36058   \n",
       "3             7.522         0.0388                    1.459  1.33095   \n",
       "4             7.427        0.03553                  1.32629  1.32261   \n",
       "5             7.406         0.0314                  1.29025  1.31826   \n",
       "6             7.378        0.02799                  1.32944  1.28017   \n",
       "7             7.364        0.03157                  1.33171  1.28907   \n",
       "8             7.286        0.03371                  1.25018  1.31967   \n",
       "9             7.284        0.04083                  1.33358  1.30923   \n",
       "10            7.278         0.0347                  1.22857  1.22393   \n",
       "11            7.226        0.04454                  0.95578  1.23788   \n",
       "12              7.2        0.03751                  1.33723  1.29704   \n",
       "13            7.187        0.04176                  1.02054  0.91451   \n",
       "14            7.119        0.03839                  1.39451  1.24711   \n",
       "15            6.983        0.04076                  0.98124  1.23287   \n",
       "16            6.946        0.03499                  1.56391  1.21963   \n",
       "17             6.94        0.03676                  1.33596  1.36948   \n",
       "18            6.937        0.03595                  1.30782  1.28566   \n",
       "19            6.901        0.03729                  1.42727  1.12575   \n",
       "20            6.867        0.01866                  1.26637  1.28548   \n",
       "21            6.853        0.05335                  1.36011  1.08182   \n",
       "22             6.81        0.06476                  1.04424  1.25596   \n",
       "23            6.798         0.0378                  1.52186     1.02   \n",
       "24            6.786         0.0491                  1.06353   1.1985   \n",
       "25             6.75        0.01848                  1.32792  1.29937   \n",
       "26             6.67          0.058                  1.10715  1.12447   \n",
       "27            6.611        0.06257                  1.69042   1.0786   \n",
       "28            6.575        0.03512                  1.27778  1.26038   \n",
       "29            6.574        0.04612                  1.05351  1.24823   \n",
       "..              ...            ...                      ...      ...   \n",
       "128           4.307        0.04351                  0.27108  0.70905   \n",
       "129           4.297        0.04221                   0.7419  0.38562   \n",
       "130           4.292         0.0613                  0.01604  0.41134   \n",
       "131           4.271        0.03751                  0.83524  1.01905   \n",
       "132           4.252        0.04678                   0.4225  0.88767   \n",
       "133           4.218        0.04828                  1.01216  1.10614   \n",
       "134           4.194         0.0326                   0.8818    0.747   \n",
       "135           4.077        0.04367                  0.54649  0.68093   \n",
       "136           4.033        0.04758                  0.75778   0.8604   \n",
       "137           3.995        0.05602                  0.26074  1.03526   \n",
       "138           3.989        0.06682                  0.67866   0.6629   \n",
       "139           3.956        0.04797                  0.23906  0.79273   \n",
       "140           3.931        0.04317                  0.21102  1.13299   \n",
       "141           3.904        0.03608                  0.36498  0.97619   \n",
       "142           3.896        0.04547                  1.06024  0.90528   \n",
       "143           3.845        0.03602                   0.0694  0.77265   \n",
       "144           3.819        0.05069                  0.46038  0.62736   \n",
       "145           3.781        0.05061                   0.2852  1.00268   \n",
       "146           3.681        0.03633                  0.20824  0.66801   \n",
       "147           3.678        0.06112                   0.0785        0   \n",
       "148           3.667         0.0383                  0.34193  0.76062   \n",
       "149           3.656         0.0359                  0.17417  0.46475   \n",
       "150           3.655        0.05141                  0.46534  0.77115   \n",
       "151           3.587        0.04324                  0.25812  0.85188   \n",
       "152           3.575        0.03084                  0.31982  0.30285   \n",
       "153           3.465        0.03464                  0.22208   0.7737   \n",
       "154            3.34        0.03656                  0.28665  0.35386   \n",
       "155           3.006        0.05015                   0.6632  0.47489   \n",
       "156           2.905        0.08658                   0.0153  0.41587   \n",
       "157           2.839        0.06727                  0.20868  0.13995   \n",
       "\n",
       "    Health(Life_Expectancy)  Freedom Trust(Government_Corruption) Generosity  \\\n",
       "0                   0.94143  0.66557                      0.41978    0.29678   \n",
       "1                   0.94784  0.62877                      0.14145     0.4363   \n",
       "2                   0.87464  0.64938                      0.48357    0.34139   \n",
       "3                   0.88521  0.66973                      0.36503    0.34699   \n",
       "4                   0.90563  0.63297                      0.32957    0.45811   \n",
       "5                   0.88911  0.64169                      0.41372    0.23351   \n",
       "6                   0.89284  0.61576                      0.31814     0.4761   \n",
       "7                   0.91087   0.6598                      0.43844    0.36262   \n",
       "8                   0.90837  0.63938                      0.42922    0.47501   \n",
       "9                   0.93156  0.65124                      0.35637    0.43562   \n",
       "10                  0.91387  0.41319                      0.07785    0.33172   \n",
       "11                  0.86027  0.63376                      0.10583    0.25497   \n",
       "12                  0.89042  0.62433                      0.18676    0.33088   \n",
       "13                  0.81444  0.48181                      0.21312    0.14074   \n",
       "14                  0.86179  0.54604                       0.1589    0.40105   \n",
       "15                  0.69702  0.49049                      0.17521    0.14574   \n",
       "16                  0.91894  0.61583                      0.37798    0.28034   \n",
       "17                  0.89533  0.61777                      0.28703    0.45901   \n",
       "18                  0.89667   0.5845                       0.2254     0.2225   \n",
       "19                  0.80925  0.64157                      0.38583    0.26428   \n",
       "20                  0.90943  0.59625                      0.32067    0.51912   \n",
       "21                  0.76276  0.63274                      0.32524    0.21542   \n",
       "22                  0.72052  0.42908                      0.11069    0.05841   \n",
       "23                  1.02525  0.54252                       0.4921    0.31105   \n",
       "24                  0.79661   0.5421                       0.0927    0.24434   \n",
       "25                  0.89186  0.61477                      0.21843    0.28214   \n",
       "26                  0.85857  0.44132                      0.12869    0.33363   \n",
       "27                  0.79733   0.6404                      0.52208    0.32573   \n",
       "28                  0.94579  0.55011                      0.20646    0.12332   \n",
       "29                  0.78723  0.44974                      0.08484    0.11451   \n",
       "..                      ...      ...                          ...        ...   \n",
       "128                 0.48246  0.44017                      0.19034    0.79588   \n",
       "129                 0.72926  0.40577                      0.38331    0.05547   \n",
       "130                 0.22562  0.43054                      0.06977    0.33128   \n",
       "131                 0.70806  0.53726                      0.09179    0.40828   \n",
       "132                 0.23402  0.49309                      0.05786    0.20618   \n",
       "133                 0.76649  0.30587                      0.00872    0.11921   \n",
       "134                 0.61712  0.17288                      0.06324    0.11291   \n",
       "135                 0.40064  0.35571                      0.07854    0.09131   \n",
       "136                 0.16683  0.10384                      0.07122    0.12344   \n",
       "137                 0.20583  0.38857                      0.12352    0.18798   \n",
       "138                 0.31051  0.41466                      0.11686    0.12388   \n",
       "139                 0.36315  0.22917                        0.199    0.17441   \n",
       "140                 0.33861  0.45727                      0.07267    0.29066   \n",
       "141                  0.4354  0.36772                      0.10713    0.20843   \n",
       "142                 0.43372  0.31914                      0.11091    0.06822   \n",
       "143                 0.29707  0.47692                      0.15639    0.19387   \n",
       "144                 0.61114  0.66246                      0.07247    0.40359   \n",
       "145                 0.38215  0.32878                      0.05747    0.34377   \n",
       "146                 0.46721  0.19184                      0.08124    0.21333   \n",
       "147                 0.06699  0.48879                      0.08289    0.23835   \n",
       "148                  0.1501  0.23501                      0.05269    0.18386   \n",
       "149                 0.24009  0.37725                      0.12139    0.28657   \n",
       "150                 0.15185  0.46866                      0.17922    0.20165   \n",
       "151                 0.27125  0.39493                      0.12832    0.21747   \n",
       "152                 0.30335  0.23414                      0.09719     0.3651   \n",
       "153                 0.42864  0.59201                      0.55191    0.22628   \n",
       "154                  0.3191   0.4845                       0.0801     0.1826   \n",
       "155                 0.72193  0.15684                      0.18906    0.47179   \n",
       "156                 0.22396   0.1185                      0.10062    0.19727   \n",
       "157                 0.28443  0.36453                      0.10731    0.16681   \n",
       "\n",
       "    Dystopia_Residual  Year  \n",
       "0             2.51738  2015  \n",
       "1             2.70201  2015  \n",
       "2             2.49204  2015  \n",
       "3             2.46531  2015  \n",
       "4             2.45176  2015  \n",
       "5             2.61955  2015  \n",
       "6              2.4657  2015  \n",
       "7             2.37119  2015  \n",
       "8             2.26425  2015  \n",
       "9             2.26646  2015  \n",
       "10            3.08854  2015  \n",
       "11            3.17728  2015  \n",
       "12             2.5332  2015  \n",
       "13            3.60214  2015  \n",
       "14            2.51011  2015  \n",
       "15            3.26001  2015  \n",
       "16            1.96961  2015  \n",
       "17             1.9757  2015  \n",
       "18            2.41484  2015  \n",
       "19            2.24743  2015  \n",
       "20            1.96994  2015  \n",
       "21            2.47489  2015  \n",
       "22            3.19131  2015  \n",
       "23            1.88501  2015  \n",
       "24            2.84848  2015  \n",
       "25            2.11569  2015  \n",
       "26            2.67585  2015  \n",
       "27            1.55674  2015  \n",
       "28            2.21126  2015  \n",
       "29              2.836  2015  \n",
       "..                ...   ...  \n",
       "128           1.41805  2015  \n",
       "129           1.59541  2015  \n",
       "130           2.80791  2015  \n",
       "131           0.67108  2015  \n",
       "132           1.95071  2015  \n",
       "133           0.89991  2015  \n",
       "134           1.59927  2015  \n",
       "135           1.92313  2015  \n",
       "136           1.94939  2015  \n",
       "137           1.79293  2015  \n",
       "138           1.68135  2015  \n",
       "139           1.95812  2015  \n",
       "140           1.42766  2015  \n",
       "141           1.44395  2015  \n",
       "142           0.99895  2015  \n",
       "143           1.87877  2015  \n",
       "144           0.98195  2015  \n",
       "145           1.38079  2015  \n",
       "146             1.851  2015  \n",
       "147            2.7223  2015  \n",
       "148           1.94296  2015  \n",
       "149           1.99172  2015  \n",
       "150           1.41723  2015  \n",
       "151           1.46494  2015  \n",
       "152            1.9521  2015  \n",
       "153           0.67042  2015  \n",
       "154           1.63328  2015  \n",
       "155           0.32858  2015  \n",
       "156           1.83302  2015  \n",
       "157           1.56726  2015  \n",
       "\n",
       "[158 rows x 13 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_1_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country 0\n",
      "Region 0\n",
      "Happiness_Rank 0\n",
      "Happiness_Score 0\n",
      "Standard_Error 0\n",
      "Economy (GDP per Capita) 0\n",
      "Family 0\n",
      "Health(Life_Expectancy) 0\n",
      "Freedom 0\n",
      "Trust(Government_Corruption) 0\n",
      "Generosity 0\n",
      "Dystopia_Residual 0\n",
      "Year 0\n"
     ]
    }
   ],
   "source": [
    "for col in list(dataset_1_data.columns):\n",
    "    print(col, dataset_1_data.filter((dataset_1_data[col] == \"\") | dataset_1_data[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country 0\n",
      "Region 0\n",
      "Happiness Rank 0\n",
      "Happiness Score 0\n",
      "Lower Confidence Interval 0\n",
      "Upper Confidence Interval 0\n",
      "Economy (GDP per Capita) 0\n",
      "Family 0\n",
      "Health (Life Expectancy) 0\n",
      "Freedom 0\n",
      "Trust (Government Corruption) 0\n",
      "Generosity 0\n",
      "Dystopia Residual 0\n",
      "Year 0\n"
     ]
    }
   ],
   "source": [
    "for col in list(dataset_2_data.columns):\n",
    "    print(col, dataset_2_data.filter((dataset_2_data[col] == \"\") | dataset_2_data[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_data.createOrReplaceTempView('dataset_1_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Happiness_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norway</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canada</td>\n",
       "      <td>North America</td>\n",
       "      <td>7.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>7.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "      <td>7.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia and New Zealand</td>\n",
       "      <td>7.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country                     Region Happiness_Score\n",
       "0  Switzerland             Western Europe           7.587\n",
       "1      Iceland             Western Europe           7.561\n",
       "2      Denmark             Western Europe           7.527\n",
       "3       Norway             Western Europe           7.522\n",
       "4       Canada              North America           7.427\n",
       "5      Finland             Western Europe           7.406\n",
       "6  Netherlands             Western Europe           7.378\n",
       "7       Sweden             Western Europe           7.364\n",
       "8  New Zealand  Australia and New Zealand           7.286\n",
       "9    Australia  Australia and New Zealand           7.284"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_2015 = sc.sql(\"\"\" SELECT Country, Region, Happiness_Score\n",
    "                                FROM dataset_1_data \n",
    "                                WHERE Happiness_Score > 6\n",
    "                                \n",
    "                                ORDER BY Happiness_Score DESC\"\"\")\n",
    "happiness_2015.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Happiness_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Togo</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>2.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>2.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syria</td>\n",
       "      <td>Middle East and Northern Africa</td>\n",
       "      <td>3.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Benin</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rwanda</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Southern Asia</td>\n",
       "      <td>3.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ivory Coast</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chad</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>3.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country                           Region Happiness_Score\n",
       "0          Togo               Sub-Saharan Africa           2.839\n",
       "1       Burundi               Sub-Saharan Africa           2.905\n",
       "2         Syria  Middle East and Northern Africa           3.006\n",
       "3         Benin               Sub-Saharan Africa            3.34\n",
       "4        Rwanda               Sub-Saharan Africa           3.465\n",
       "5   Afghanistan                    Southern Asia           3.575\n",
       "6  Burkina Faso               Sub-Saharan Africa           3.587\n",
       "7   Ivory Coast               Sub-Saharan Africa           3.655\n",
       "8        Guinea               Sub-Saharan Africa           3.656\n",
       "9          Chad               Sub-Saharan Africa           3.667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_2015 = sc.sql(\"\"\" SELECT Country, Region, Happiness_Score\n",
    "                                FROM dataset_1_data \n",
    "                                ORDER BY Happiness_Score\"\"\")\n",
    "happiness_2015.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_2015.write.format('com.databricks.spark.csv').save('./my.csv',header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = pymongo.MongoClient(\"mongodb+srv://dbUser:<pass>@cluster0.t72w2.mongodb.net/myfirstdatabase?retryWrites=true&w=majority\")\n",
    "db = client.test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['cluster0-shard-00-01.t72w2.mongodb.net:27017', 'cluster0-shard-00-00.t72w2.mongodb.net:27017', 'cluster0-shard-00-02.t72w2.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', authsource='admin', replicaset='atlas-o9qwvt-shard-0', ssl=True), 'test'), 'test-collection')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = client[\"firstdatabase\"]\n",
    "mycol = mydb[\"customers\"]\n",
    "\n",
    "mydict = { \"name\": \"John\", \"address\": \"Highway 37\" }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = { \"name\": \"John\", \"address\": \"Highway 37\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mycol.insert_one(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple posts: [ObjectId('5f2bcdfdd8def81850e9e781'), ObjectId('5f2bcdfdd8def81850e9e782'), ObjectId('5f2bcdfdd8def81850e9e783')]\n"
     ]
    }
   ],
   "source": [
    "post_1 = {\n",
    "    'title': 'Python and MongoDB',\n",
    "    'content': 'PyMongo is fun, you guys',\n",
    "    'author': 'Scott'\n",
    "}\n",
    "post_2 = {\n",
    "    'title': 'Virtual Environments',\n",
    "    'content': 'Use virtual environments, you guys',\n",
    "    'author': 'Scott'\n",
    "}\n",
    "post_3 = {\n",
    "    'title': 'Learning Python',\n",
    "    'content': 'Learn Python, it is easy',\n",
    "    'author': 'Bill'\n",
    "}\n",
    "new_result = mycol.insert_many([post_1, post_2, post_3])\n",
    "print('Multiple posts: {0}'.format(new_result.inserted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5f2bcca4d8def81850e9e77d'), 'title': 'Learning Python', 'content': 'Learn Python, it is easy', 'author': 'Bill'}\n"
     ]
    }
   ],
   "source": [
    "bills_post = mycol.find_one({'author': 'Bill'})\n",
    "print(bills_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycol = mydb[\"customers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "# conf = SparkConf().setAppName(\"test\").setMaster(\"local\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    " \n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sqlContext.read.format('com.databricks.spark.csv').options(header='false', inferschema='true').load('2013_data_tiny.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, _c1: int, _c2: int, _c3: int, _c4: int, _c5: int, _c6: int, _c7: string, _c8: int, _c9: string, _c10: string, _c11: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+-----+---+----+-------------+---------+----+\n",
      "| _c0|_c1|_c2|_c3|_c4|_c5|  _c6|_c7| _c8|          _c9|     _c10|_c11|\n",
      "+----+---+---+---+---+---+-----+---+----+-------------+---------+----+\n",
      "|2013|  1|  1|  0|  0| 10|42101| co| 628| C628_42101_1|235.77068|null|\n",
      "|2013|  1|  1|  0|  0| 10|42101| co|1035|C1035_42101_1|229.49535|null|\n",
      "|2013|  1|  1|  0|  5| 10|42101| co| 628| C628_42101_1|221.14418|null|\n",
      "|2013|  1|  1|  0|  5| 10|42101| co|1035|C1035_42101_1|231.42252|null|\n",
      "|2013|  1|  1|  0| 10| 10|42101| co| 628| C628_42101_1|357.65799|null|\n",
      "|2013|  1|  1|  0| 10| 10|42101| co|1035|C1035_42101_1|233.34969|null|\n",
      "|2013|  1|  1|  0| 15| 10|42101| co| 628| C628_42101_1| 89.50581|null|\n",
      "|2013|  1|  1|  0| 15| 10|42101| co|1035|C1035_42101_1|229.97713|null|\n",
      "|2013|  1|  1|  0| 20| 10|42101| co| 628| C628_42101_1| 84.63032|null|\n",
      "|2013|  1|  1|  0| 20| 10|42101| co|1035|C1035_42101_1|229.01355|null|\n",
      "|2013|  1|  1|  0| 25| 10|42101| co| 628| C628_42101_1|152.88725|null|\n",
      "|2013|  1|  1|  0| 25| 10|42101| co|1035|C1035_42101_1|231.42252|null|\n",
      "|2013|  1|  1|  0| 30| 10|42101| co| 628| C628_42101_1|196.76671|null|\n",
      "|2013|  1|  1|  0| 30| 10|42101| co|1035|C1035_42101_1|225.64098|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3|   2|   C2_44201_2|  8.09507|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3|   9|   C9_44201_2| 25.60723|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3|  28|  C28_44201_1| 14.95503|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3|  64|  C64_44201_1| 29.53309|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3| 628| C628_44201_1| 28.24655|null|\n",
      "|2013|  1|  1|  0|  0| 10|44201| o3| 640| C640_44201_1|  32.1266|null|\n",
      "+----+---+---+---+---+---+-----+---+----+-------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---+------+-----------+----+----+-------------+---------+----+\n",
      "|year|month|day|hour|min|region|parmater_id|name|site|         cams|    value|flag|\n",
      "+----+-----+---+----+---+------+-----------+----+----+-------------+---------+----+\n",
      "|2013|    1|  1|   0|  0|    10|      42101|  co| 628| C628_42101_1|235.77068|null|\n",
      "|2013|    1|  1|   0|  0|    10|      42101|  co|1035|C1035_42101_1|229.49535|null|\n",
      "|2013|    1|  1|   0|  5|    10|      42101|  co| 628| C628_42101_1|221.14418|null|\n",
      "|2013|    1|  1|   0|  5|    10|      42101|  co|1035|C1035_42101_1|231.42252|null|\n",
      "|2013|    1|  1|   0| 10|    10|      42101|  co| 628| C628_42101_1|357.65799|null|\n",
      "|2013|    1|  1|   0| 10|    10|      42101|  co|1035|C1035_42101_1|233.34969|null|\n",
      "|2013|    1|  1|   0| 15|    10|      42101|  co| 628| C628_42101_1| 89.50581|null|\n",
      "|2013|    1|  1|   0| 15|    10|      42101|  co|1035|C1035_42101_1|229.97713|null|\n",
      "|2013|    1|  1|   0| 20|    10|      42101|  co| 628| C628_42101_1| 84.63032|null|\n",
      "|2013|    1|  1|   0| 20|    10|      42101|  co|1035|C1035_42101_1|229.01355|null|\n",
      "|2013|    1|  1|   0| 25|    10|      42101|  co| 628| C628_42101_1|152.88725|null|\n",
      "|2013|    1|  1|   0| 25|    10|      42101|  co|1035|C1035_42101_1|231.42252|null|\n",
      "|2013|    1|  1|   0| 30|    10|      42101|  co| 628| C628_42101_1|196.76671|null|\n",
      "|2013|    1|  1|   0| 30|    10|      42101|  co|1035|C1035_42101_1|225.64098|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3|   2|   C2_44201_2|  8.09507|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3|   9|   C9_44201_2| 25.60723|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3|  28|  C28_44201_1| 14.95503|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3|  64|  C64_44201_1| 29.53309|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3| 628| C628_44201_1| 28.24655|null|\n",
      "|2013|    1|  1|   0|  0|    10|      44201|  o3| 640| C640_44201_1|  32.1266|null|\n",
      "+----+-----+---+----+---+------+-----------+----+----+-------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- min: integer (nullable = true)\n",
      " |-- region: integer (nullable = true)\n",
      " |-- parmater_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- site: integer (nullable = true)\n",
      " |-- cams: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df1.selectExpr(\" _c0 as year\", \"_c1 as month\",\"_c2 as day\",\"_c3 as hour\", \"_c4 as min\", \"_c5 as region\",\"_c6 as parmater_id\", \"_c7 as name\",\"_c8 as site\",\"_c9 as cams\",\"_c10 as value\",\"_c11 as flag\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.coalesce(1).write.format('json').save('file_name.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
